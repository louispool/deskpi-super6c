# Variables used by the playbooks
---
# Whether Ubuntu Server was installed as the OS
ubuntu_os: true
# Whether to format the available SD card to ext4
format_sdcard: true
# Whether to reduce the memory allocated to the GPU
reduce_gpu_mem: false
# This tells the firmware of the CM5 “I know my PSU can handle it, skip your safety limit.”
disable_cm5_pd_current_limit: true

# k3s config directory
k3s_config_dir: /etc/rancher/k3s

# Kubelet configuration - enables Graceful Node Shutdown (see https://kubernetes.io/blog/2021/04/21/graceful-node-shutdown-beta/)
k3s_kubelet_config: |
  apiVersion: kubelet.config.k8s.io/v1beta1
  kind: KubeletConfiguration
  shutdownGracePeriod: 30s
  shutdownGracePeriodCriticalPods: 10s

# Extra arguments for k3s server installation
#
# `--write-kubeconfig-mode '0644'` gives read permissions to kubeconfig file (located at /etc/rancher/k3s/k3s.yaml)
# `--disable servicelb` disables the default service load balancer installed by k3s (i.e. Klipper Load Balancer)
# `--disable traefik` disables the default ingress controller installed by k3s (i.e. Traefik)
# `--kubelet-arg 'config=/etc/rancher/k3s/kubelet.config'` points to the kubelet configuration (see above).
# `--kube-scheduler-arg 'bind-address=0.0.0.0'` exposes the 0.0.0.0 address endpoint on the Kube Scheduler for metrics scraping.
# `--kube-proxy-arg 'metrics-bind-address=0.0.0.0'` exposes the 0.0.0.0 address endpoint on the Kube Proxy for metrics scraping.
# `--kube-controller-manager-arg 'bind-address=0.0.0.0'` exposes the 0.0.0.0 address endpoint on the Kube Controller Manager for metrics scraping.
# `--kube-controller-manager-arg 'terminated-pod-gc-threshold=10'` set a limit of 10 terminated pods that can exist before the garbage collector starts deleting terminated pods
#                                                                  (see https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-garbage-collection)
k3s_server_extra_args: >-
  --write-kubeconfig-mode '0644'
  --disable 'servicelb'
  --disable 'traefik'
  --kubelet-arg 'config=/etc/rancher/k3s/kubelet.config'
  --kube-proxy-arg 'metrics-bind-address=0.0.0.0'
  --kube-scheduler-arg 'bind-address=0.0.0.0'
  --kube-controller-manager-arg 'bind-address=0.0.0.0'
  --kube-controller-manager-arg 'terminated-pod-gc-threshold=10'

# Extra arguments for k3s agent installation
#
# `--node-label 'node_type=worker'`adds a custom label to the worker node.
# `--kubelet-arg 'config=/etc/rancher/k3s/kubelet.config'` points to the kubelet configuration (see above).
# `--kube-proxy-arg 'metrics-bind-address=0.0.0.0'` exposes the 0.0.0.0 address endpoint on the Kube Proxy for metrics scraping.
k3s_worker_extra_args: >-
  --node-label 'node_type=worker'
  --kubelet-arg 'config=/etc/rancher/k3s/kubelet.config'
  --kube-proxy-arg 'metrics-bind-address=0.0.0.0'

# Namespaces
k3s_metallb_namespace: metallb-system
k3s_traefik_namespace: traefik-system
k3s_cert_manager_namespace: cert-manager-system
k3s_longhorn_namespace: longhorn-system
k3s_linkerd_namespace: linkerd-control-plane
k3s_linkerd_viz_namespace: linkerd-viz
k3s_logging_namespace: logging
k3s_monitoring_namespace: monitoring
k3s_opensearch_namespace: opensearch
k3s_minio_operator_namespace: minio-operator
k3s_minio_tenant_namespace: minio-tenant

# Range of IP addresses that MetalLB can use to assign IPs to services
k3s_external_min_ip_octet: 100
k3s_external_max_ip_octet: 199
# In either CIDR notation (e.g. 192.168.10.0/24) or a start and an end IP address separated by a hyphen (192.168.9.1-192.168.9.5)
k3s_external_ip_range: "{{ ip_subnet_prefix|string + '.' + k3s_external_min_ip_octet|string + '-' + ip_subnet_prefix|string +  '.' + k3s_external_max_ip_octet|string }}"

# External IP for Kubernetes Ingress (used by Traefik)
k3s_ingress_external_ip: "{{ ip_subnet_prefix|string + '.' + k3s_external_min_ip_octet|string }}"

# Whether to install Linkerd and inject proxy annotations into deployments
k3s_enable_service_mesh: false
# Whether to install Longhorn
k3s_enable_block_storage: true
# Whether to install the kube-prometheus-stack for monitoring
k3s_enable_monitoring: true
# Whether to install OpenSearch and FluentBit for logging
k3s_enable_logstack: true

# IP address of the NTP Server
ntp_server: "{{ master_node_ip }}"
# IP Address of the DNS Server
dns_server: "{{ master_node_ip }}"
# IP address dnsmasq should listen on
dnsmasq_listen_address: "{{ master_node_ip }}"

# Whether an NFS Service should be configured on the Host of the control plane
local_nfs_server: true
nfs_dir: /NFS/data
# IP address of the NFS Server (leave empty if `local_nfs_server` is true)
nfs_server:

# Name of the Storage Class for the NFS provisioner
nfs_storage_class_name: nfs-client

# Public domain - top level domain on the Internet
public_domain:
# Subdomain of the cluster on the Internet
cluster_public_domain: deskpi.{{ public_domain }}

# Top level domain for the Local Network
local_domain: local
# Subdomain of the cluster on the Local Network
cluster_local_domain: deskpi.{{ local_domain }}

# Additional DNS entries for dnsmasq
base_additional_dns_hosts:
  ntp_server:
    desc: "NTP Server"
    hostname: ntp
    ip: "{{ ntp_server }}"
  dns_server:
    desc: "DNS Server"
    hostname: dns
    ip: "{{ dns_server }}"

nas_additional_dns_host:
  nas_server:
    desc: "NAS Server"
    hostname: nas
    ip: "{{ master_node_ip }}"

additional_dns_hosts: >-
  {{
    base_additional_dns_hosts
    | combine( nas_additional_dns_host if local_nfs_server else {} )
  }}

# DHCP range used by dnsmasq (should not overlap range of IP addresses used by MetalLB)
dhcp_range_min_ip_octet: 200
dhcp_range_max_ip_octet: 254
dhcp_range: "{{ ip_subnet_prefix|string +  '.' + dhcp_range_min_ip_octet|string + ',' + ip_subnet_prefix|string + '.' + dhcp_range_max_ip_octet|string  }}"

enable_dhcp: false

# Service end-points
traefik_dashboard: "traefik.{{ cluster_local_domain }}"
longhorn_dashboard: "longhorn.{{ cluster_local_domain }}"
prometheus_dashboard: "prometheus.{{ cluster_local_domain }}"
alertmanager_dashboard: "alertmanager.{{ cluster_local_domain }}"
grafana_dashboard: "grafana.{{ cluster_local_domain }}"
opensearch_dashboard: "opensearch-dashboard.{{ cluster_local_domain }}"
opensearch_rest_api: "opensearch.{{ cluster_local_domain }}"
linkerd_dashboard: "linkerd.{{ cluster_local_domain }}"
minio_console: "minio.{{ cluster_local_domain }}"

public_traefik_dashboard: "traefik.{{ cluster_public_domain }}"
public_longhorn_dashboard: "longhorn.{{ cluster_public_domain }}"
public_opensearch_dashboard: "opensearch.{{ cluster_public_domain }}"
public_grafana_dashboard: "grafana.{{ cluster_public_domain }}"
public_minio_console: "minio.{{ cluster_public_domain }}"

# List of public sub-domains to register with the DNS (e.g. Route53)
public_domains:
  - "{{ public_traefik_dashboard }}"
  - "{{ public_longhorn_dashboard }}"
  - "{{ public_opensearch_dashboard }}"
  - "{{ public_grafana_dashboard }}"
  - "{{ public_minio_console }}"

# Whether to allow access to the dashboards over the public network
enable_public_traefik_dashboard: false
enable_public_longhorn_dashboard: false
enable_public_grafana_dashboard: false
enable_public_opensearch_dashboard: false
enable_public_minio_console: false

# Enable the self-signed Certificate Issuer
enable_selfsigned: true
# Enable `Let's Encrypt` ACME (Automated Certificate Management Environment) provider
enable_letsencrypt: false

# Name of the certificate authority (CA) for self-signed certificates on the Local Network
private_ca: k3s-ca
# Name of the certificate authority (CA) for certificates on the Internet
public_ca: letsencrypt

# Issuer of TLS certificates on the Local Network (e.g. self-signed)
local_tls_issuer: "{{ private_ca }}-issuer"
# Issuer of TLS certificates on the Internet (e.g. 'Let's Encrypt')
public_tls_issuer: "{{ public_ca }}-issuer"

# Valid E-mail address for registration with `Let's Encrypt`
email:

# Enable AWS' `Route 53` DNS provider for routing public traffic to the cluster ingress
enable_route53: false

# AWS configuration, necessary for Route53 DNS provider (recommended to specify/override in the vault)
aws_access_key_id:
aws_secret_access_key:
aws_region: us-east-1

# When using the Route 53 DNS provider, replace with your AWS Hosted Zone ID.
aws_hosted_zone_id:
# AWS account ID (e.g. "002782789222" from `arn:aws:iam::002782789222:user/myuser`)
aws_account_id:
# IAM username (e.g. "myuser" from `arn:aws:iam::002782789222:user/myuser`)
aws_iam_user:

# Name of the disk device for use with Longhorn (check with `lsblk -f`)
longhorn_disk_device: nvme0n1
# Path to mount the disk to (and subsequently used by Longhorn)
longhorn_storage_path: /mnt/storage
# Whether to allow wiping and formatting the disk device
longhorn_prepare_disk: false

# Storage class for Prometheus persistent storage
prom_storage_class: "longhorn"
# Size of the Persistent Volume Claim (PVC) for Prometheus data storage
prom_storage_size: "30Gi"
# Retention period for Prometheus data. This is the time period for which Prometheus will retain data before deleting it.
prom_retention: "7d"

# Size at which the log index will be rotated
log_rollover_size: "1gb"
# How long to keep the logs before they are deleted
log_retention_period: "5d"

# From these variables Kubernetes will create resources named `opensearch-cluster-master`
opensearch_cluster_name: "opensearch-cluster"
opensearch_node_group: "master"

# Last version to work with Raspberry Pi 4 is 2.9.0
opensearch_app_version: "2.19.3"
opensearch_chart_version: "2.35.0"
# Size of the OpenSearch cluster
opensearch_replicas: 3

# Configuration properties thatr define the resource requirements of the OpenSearch pods
opensearch_resources_limits_memory: "8Gi"
opensearch_resources_limits_cpu: "3000m"

opensearch_resources_requests_memory: "2Gi"
opensearch_resources_requests_cpu: "1000m"

# Configures the size of the Persistent Volume Claim (PVC) for OpenSearch data storage
opensearch_pvc_size: "100Gi"
opensearch_storage_class: "longhorn"

# Will taint the following k8s nodes with the key `opensearch=true:NoSchedule`
# if `opensearch_enable_node_tolerations` is set to true.
opensearch_nodes:
  - deskpi2
  - deskpi3
  - deskpi4

# Set to true if you want to apply a taint to certain nodes in your cluster to
# repel general workloads and reserve those nodes for OpenSearch via
# `kubectl taint nodes <node-name> opensearch=true:NoSchedule`
opensearch_enable_node_tolerations: false

